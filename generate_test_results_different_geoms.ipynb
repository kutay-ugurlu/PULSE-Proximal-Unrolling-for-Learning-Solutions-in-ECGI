{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import * \n",
    "from model_utils import *\n",
    "from visualization import *\n",
    "from os.path import join\n",
    "from dataset import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pymatreader import read_mat\n",
    "from matplotlib import pyplot as plt \n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import pandas as pd \n",
    "from utils import *\n",
    "import seaborn as sns\n",
    "import matlab.engine \n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here keep the best model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = ['Regs\\\\2024_2_2_17_25AbsUsed_1_Unrolling_1_model_LSTM_UNet.pt',\n",
    "            'Regs\\\\2023_8_12_3_6AbsUsed_1_Unrolling_1_model_LSTM_UNet.pt',\n",
    "            'Regs\\\\2023_6_13_8_16AbsUsed_1_Unrolling_1_model_LSTM_UNet.pt',\n",
    "            'Regs\\\\2023_6_22_21_12AbsUsed_1_Unrolling_1_model_LSTM_UNet.pt',\n",
    "            'Regs\\\\2023_6_22_12_22AbsUsed_1_Unrolling_1_model_LSTM_UNet.pt']\n",
    "\n",
    "reordering = read_mat('newnode_order_3.mat')['node_order'].astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_IDX = -1\n",
    "# file = get_the_latest_file_in_path(join('Regs','*.pt'),EXP_IDX)\n",
    "file = best_models[0]\n",
    "print(file)\n",
    "load_results = torch.load(file)\n",
    "model_class = load_results['modelclass']\n",
    "model = model_class(**load_results['essential_dict']).to(device)\n",
    "\n",
    "model.load_state_dict(load_results['model'])\n",
    "model.eval()\n",
    "reg_params = load_results['reg_params']\n",
    "\n",
    "f = visualize_results(file);\n",
    "f.suptitle(\"Bi-LSTM Spatial UNet\",fontweight=\"bold\",fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is for the original same geometry reconstructions\n",
    "\n",
    "# A = read_mat(\"ForwMat_HT.mat\")\n",
    "# A = torch.from_numpy(A['Trf_HT_leads']).to(device)\n",
    "# A = A[:, reordering-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the test data you want to apply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 30\n",
    "direction = 'z'\n",
    "angle = '21'\n",
    "TEST_data = torch.load(join(\"Test_Dictionaries\",\"test_dictionary_\"+direction+\"_\"+str(angle)+\"_Noise\"+str(noise)+\".pt\")) #torch.load('test_dictionary.pt')\n",
    "A = torch.tensor(TEST_data[0]['inverse_matrix']).to(device)\n",
    "A_np = A.to('cpu').numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beat by beat, uneven batch/time samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_param = load_results[\"reg_params\"]\n",
    "BATCH_SIZE = 64 if not 'batch_size' in load_results['essential_dict'].keys() else load_results['essential_dict']['batch_size'] \n",
    "results = test_model(model, reg_params, TEST_data, A, device, use_abs=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([results[str(i)][\"AvgTime\"] for i in range(17)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [('Test Data',''),\n",
    "           ('Spatial CC', 'Proposed'), ('Spatial CC', 'MAP'),('Spatial CC', 'Tik'),\n",
    "           ('Temporal CC', 'Proposed'), ('Temporal CC', 'MAP'),('Temporal CC', 'Tik'),\n",
    "            ('Spatial RE', 'Proposed'), ('Spatial RE', 'MAP'), ('Spatial RE', 'Tik'),\n",
    "           ('Temporal RE', 'Proposed'), ('Temporal RE', 'MAP'),('Temporal RE', 'Tik'),\n",
    "           ('AT CC', 'Proposed'), ('AT CC', 'MAP'),('AT CC', 'Tik'),\n",
    "           ('Localization Error', 'Proposed'), ('Localization Error', 'MAP'),('Localization Error', 'Tik'),\n",
    "           ]\n",
    "df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(columns), data=np.round(np.zeros((18,19)),3))\n",
    "\n",
    "test_data_col = np.arange(1,18).tolist() + [\"Median (IQR)\"]\n",
    "df[\"Test Data\"] = test_data_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = glob(join('TestData','EP','*.mat'))\n",
    "training_files = np.array(glob('TrainingData/*.mat'))\n",
    "train_idx, validation_idx = train_test_split(np.arange(0,training_files.__len__()),random_state=101,test_size=0.05)\n",
    "training_files = training_files[train_idx]\n",
    "training_files.__len__()\n",
    "\n",
    "training_data = np.empty((490,0))\n",
    "for item in training_files:\n",
    "    data = read_mat(item)\n",
    "    qrs_begin = data['features']['QRSbegin'] - 1\n",
    "    qrs_end = data['features']['QRSend'] - 1 \n",
    "    potvals = data['ts']['potvals'][reordering,qrs_begin:qrs_end]\n",
    "    training_data = np.hstack((training_data,potvals))\n",
    "    \n",
    "training_data = training_data[:,1:]\n",
    "print(training_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cx, meanX = create_moments(training_data)\n",
    "Cx.shape , meanX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = np.empty_like((1,))\n",
    "metric_names = np.empty_like((1,))\n",
    "test_data = np.empty_like((1,))\n",
    "method = np.empty_like((1,))\n",
    "\n",
    "for i in tqdm(range(17)):\n",
    "\n",
    "    data = TEST_data[i]\n",
    "    x = data['x'].numpy()\n",
    "    y = data['y'].numpy()\n",
    "    badleads = data['badleads']\n",
    "    std_n = data['std_n']\n",
    "    at = data['at']\n",
    "    paceloc = data['paceloc']\n",
    "    Cn = std_n**2*np.eye(y.shape[0])\n",
    "    \n",
    "    x_hat = map_solution(A_np,Cx,Cn,y,meanX) \n",
    "    spat_cc, spat_re, temp_cc, temp_re = metrics_with_bad_leads(torch.Tensor(x.T),torch.Tensor(x_hat.T),badleads)\n",
    "    AT_Results = eng.AT_and_LE(x_hat,paceloc.item(),np.asmatrix(at).T,badleads.cpu().numpy()-1,nargout=2)\n",
    "    \n",
    "\n",
    "    \n",
    "    metrics_for_test_data = np.concatenate((spat_cc.flatten(), spat_re.flatten(), temp_cc.flatten(), temp_re.flatten()))\n",
    "    metric_names_for_test_data = np.concatenate((np.array(spat_cc.size*['Spatial CC']),np.array(spat_re.size*['Spatial RE']),\n",
    "                    np.array(temp_cc.size*['Temporal CC']),np.array(temp_re.size*['Temporal RE'])))\n",
    "    testdata_for_test_data = (i+1)*np.ones(metric_names_for_test_data.shape)\n",
    "    solution_method = np.array(metric_names_for_test_data.size*['MAP'])\n",
    "\n",
    "    metrics = np.concatenate((metrics,metrics_for_test_data))\n",
    "    metric_names = np.concatenate((metric_names,metric_names_for_test_data))\n",
    "    test_data = np.concatenate((test_data,testdata_for_test_data))\n",
    "    method = np.concatenate((method ,solution_method))\n",
    "    \n",
    "    df.loc[i,(\"Spatial CC\",\"MAP\")] = format(np.median(spat_cc),'.3f') + ' (' + format(np.subtract(*np.percentile(spat_cc,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"Spatial RE\",\"MAP\")] = format(np.median(spat_re),'.3f') + ' (' + format(np.subtract(*np.percentile(spat_re,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"Temporal CC\",\"MAP\")] = format(np.median(temp_cc),'.3f') + ' (' + format(np.subtract(*np.percentile(temp_cc,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"Temporal RE\",\"MAP\")] = format(np.median(temp_re),'.3f') + ' (' + format(np.subtract(*np.percentile(temp_re,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"AT CC\",\"MAP\")] = format(AT_Results[0],'.3f')\n",
    "    df.loc[i,(\"Localization Error\",\"MAP\")] = format(AT_Results[1],'.3f')\n",
    "    \n",
    "    results[str(i)][\"MAP\"] = x_hat\n",
    "    \n",
    "    ##############################################################################################################################################\n",
    "    \n",
    "    x_hat = DFBlock(A.to(device),torch.Tensor([1.75e-4]).to(device),torch.from_numpy(y).to(device),torch.zeros_like(torch.from_numpy(x).to(device)),device,use_abs=True)\n",
    "    spat_cc, spat_re, temp_cc, temp_re = metrics_with_bad_leads(torch.Tensor(x.T).to(device),torch.Tensor(x_hat.T).to(device),badleads)\n",
    "    AT_Results = eng.AT_and_LE(x_hat.detach().cpu().numpy(),paceloc.item(),np.asmatrix(at).T,badleads.cpu().numpy()-1,nargout=2)\n",
    "    \n",
    "    metrics_for_test_data = np.concatenate((spat_cc.flatten(), spat_re.flatten(), temp_cc.flatten(), temp_re.flatten()))\n",
    "    metric_names_for_test_data = np.concatenate((np.array(spat_cc.size*['Spatial CC']),np.array(spat_re.size*['Spatial RE']),\n",
    "                    np.array(temp_cc.size*['Temporal CC']),np.array(temp_re.size*['Temporal RE'])))\n",
    "    testdata_for_test_data = (i+1)*np.ones(metric_names_for_test_data.shape)\n",
    "    solution_method = np.array(metric_names_for_test_data.size*['Tik'])\n",
    "\n",
    "    metrics = np.concatenate((metrics,metrics_for_test_data))\n",
    "    metric_names = np.concatenate((metric_names,metric_names_for_test_data))\n",
    "    test_data = np.concatenate((test_data,testdata_for_test_data))\n",
    "    method = np.concatenate((method ,solution_method))\n",
    "    \n",
    "    df.loc[i,(\"Spatial CC\",\"Tik\")] = format(np.median(spat_cc),'.3f') + ' (' + format(np.subtract(*np.percentile(spat_cc,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"Spatial RE\",\"Tik\")] = format(np.median(spat_re),'.3f') + ' (' + format(np.subtract(*np.percentile(spat_re,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"Temporal CC\",\"Tik\")] = format(np.median(temp_cc),'.3f') + ' (' + format(np.subtract(*np.percentile(temp_cc,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"Temporal RE\",\"Tik\")] = format(np.median(temp_re),'.3f') + ' (' + format(np.subtract(*np.percentile(temp_re,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"AT CC\",\"Tik\")] = format(AT_Results[0],'.3f')\n",
    "    df.loc[i,(\"Localization Error\",\"Tik\")] = format(AT_Results[1],'.3f')\n",
    "    \n",
    "    results[str(i)][\"Tik\"] = x_hat.detach().cpu().numpy()\n",
    "        \n",
    "    ##############################################################################################################################################\n",
    "    \n",
    "    temp_cc = results[str(i)]['temp_cc']\n",
    "    temp_re = results[str(i)]['temp_re']\n",
    "    spat_cc = results[str(i)]['spat_cc']\n",
    "    spat_re = results[str(i)]['spat_re']\n",
    "    LE = results[str(i)]['LE']\n",
    "    ATCC = results[str(i)]['AT_CC']\n",
    "\n",
    "    \n",
    "    metrics_for_test_data = np.concatenate((spat_cc.flatten(), spat_re.flatten(), temp_cc.flatten(), temp_re.flatten()))\n",
    "    metric_names_for_test_data = np.concatenate((np.array(spat_cc.size*['Spatial CC']),np.array(spat_re.size*['Spatial RE']),\n",
    "                    np.array(temp_cc.size*['Temporal CC']),np.array(temp_re.size*['Temporal RE'])))\n",
    "    testdata_for_test_data = (i+1)*np.ones(metric_names_for_test_data.shape)\n",
    "    solution_method = np.array(metric_names_for_test_data.size*['NN'])\n",
    "\n",
    "    metrics = np.concatenate((metrics,metrics_for_test_data))\n",
    "    metric_names = np.concatenate((metric_names,metric_names_for_test_data))\n",
    "    test_data = np.concatenate((test_data,testdata_for_test_data))\n",
    "    method = np.concatenate((method ,solution_method))\n",
    "    \n",
    "    \n",
    "    df.loc[i,(\"Spatial CC\",\"Proposed\")] = format(np.median(spat_cc),'.3f') + ' (' + format(np.subtract(*np.percentile(spat_cc,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"Spatial RE\",\"Proposed\")] = format(np.median(spat_re),'.3f') + ' (' + format(np.subtract(*np.percentile(spat_re,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"Temporal CC\",\"Proposed\")] = format(np.median(temp_cc),'.3f') + ' (' + format(np.subtract(*np.percentile(temp_cc,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"Temporal RE\",\"Proposed\")] = format(np.median(temp_re),'.3f') + ' (' + format(np.subtract(*np.percentile(temp_re,[75, 25])),'.3f') + ')'\n",
    "    df.loc[i,(\"AT CC\",\"Proposed\")] = format(ATCC,'.3f')\n",
    "    df.loc[i,(\"Localization Error\",\"Proposed\")] = format(LE,'.3f')\n",
    "    \n",
    "    results[str(i)][\"GT\"] = x\n",
    "    results[str(i)][\"y\"] = y\n",
    "    \n",
    "file=\"TestResults//ResultsDict_Model_\"+direction+\"_\"+str(angle)+\"_\"+'_Noise'+ str(noise) + file.split(\".\")[0].split(\"//\")[1]\n",
    "np.save(file=file,arr=results,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.vstack((metric_names[1:],\n",
    "            test_data[1:].astype(int),\n",
    "            np.round(metrics[1:],3).astype(float),\n",
    "            method[1:]))\n",
    "data.shape\n",
    "columns = ['Metrics','TestData','Values','Method']\n",
    "df2 = pd.DataFrame(columns=columns, data=data.T)\n",
    "df2['Values'] = pd.to_numeric(df2['Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'serif',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(40,25))\n",
    "\n",
    "flierprops = dict(markersize=5,marker='o',\n",
    "              linestyle='none')\n",
    "\n",
    "for i,selected_metric in enumerate(['Temporal CC','Spatial CC','Temporal RE','Spatial RE']):\n",
    "    subdf = df2.loc[df2['Metrics']==selected_metric]\n",
    "    subdf.head()\n",
    "    plt.figure(figsize=(20,12))\n",
    "    # plt.subplot(2,2,i+1)\n",
    "    sns.boxplot(data=subdf, x='TestData',y='Values',hue='Method',flierprops=flierprops,showfliers=False)\n",
    "    plt.ylabel(selected_metric,fontweight='bold')\n",
    "    plt.xlabel('Test Data Index',fontweight='bold')\n",
    "    \n",
    "    if selected_metric == 'Temporal RE':\n",
    "        plt.ylim([0, 2])\n",
    "        \n",
    "    elif selected_metric == 'Spatial CC':\n",
    "        plt.ylim([-0.3, 1])\n",
    "\n",
    "    plt.savefig('TestResults//'+str(BATCH_SIZE)+'_dropout_shuffled_'+'_Noise'+ str(noise) + selected_metric.replace(\" \",\"\") + \"_\" + direction + \"_\" + str(angle) + \"_\" + \n",
    "                '.pdf', format = 'pdf', dpi=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[17,1:13] = \"-\"\n",
    "medians = df.iloc[:17,13:].astype(float).median().round(3)\n",
    "iqrs = df.iloc[:17,13:].astype(float).apply(lambda x: x.quantile(0.75) - x.quantile(0.25)).round(3)\n",
    "\n",
    "total_col = medians.astype(str) + \"(\" + iqrs.astype(str) + \")\"\n",
    "df.iloc[17,13:] = total_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = [df.iloc[1:17,i].apply(lambda x: x.split(\"(\")[0]).astype(float).round(3).median() for i in range(1,13)]\n",
    "iqrs = [df.iloc[1:17,i].apply(lambda x: x.split(\"(\")[0]).astype(float).round(3).quantile(0.75) - df.iloc[1:17,i].apply(lambda x: x.split(\"(\")[0]).astype(float).round(3).quantile(0.25)  for i in range(1,13)]\n",
    "\n",
    "df.iloc[17,1:13] = [str(round(item1,3)) + \"(\" + str(round(item2,3)) + \")\" for item1,item2 in zip(medians,iqrs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_df = df.style.set_table_styles([\n",
    "    {'selector': 'th', 'props': [('text-align', 'center')]}\n",
    "])\n",
    "# Display the styled DataFrame\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style.hide(axis=\"index\").to_latex('TestResults//'+str(BATCH_SIZE) + '_dropout_shuffled' + direction + \"_\" + str(angle) +'_Noise'+ str(noise) +'.tex',column_format=df.columns.__len__()*'c')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visualization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
